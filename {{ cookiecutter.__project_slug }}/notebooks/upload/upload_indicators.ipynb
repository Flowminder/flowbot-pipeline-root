{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f158709",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e58c39-0b76-4e3c-82e0-7420d339c4c3",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "from collections.abc import Iterator\n",
    "from pathlib import Path\n",
    "from httpx import ReadTimeout, TimeoutException\n",
    "import os\n",
    "import httpx\n",
    "import logging\n",
    "\n",
    "from auth import get_token\n",
    "from params import (\n",
    "    SridParams,\n",
    "    TridParams,\n",
    "    CategoryParams,\n",
    "    lookup_factory,\n",
    "    upload_config,\n",
    "    get_remote_parameter,\n",
    "    get_local_parameter,\n",
    ")\n",
    "from upload import do_upload, AttemptState, set_scopes, store_mdids\n",
    "from data_cleaning import JsonDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c921b66-4b12-4034-8a22-c7719475228f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UploadError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "class HeartbeatError(Exception):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c41419-df25-46dd-81dd-077bd40a7772",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    for line in Path(\"../.env\").read_text().splitlines():\n",
    "        foo, _, bar = line.partition(\"=\")\n",
    "        os.environ[foo] = bar\n",
    "except FileNotFoundError:\n",
    "    # No env file\n",
    "    pass\n",
    "\n",
    "log = logging.getLogger(\"upload_notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13398e18-5fd1-4c89-9b91-7a289dc6ed82",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "shared_data_dir = Path(\n",
    "    \"../tests/\"\n",
    ")  # The path to the shared_data_dir for this notebook (see FlowpyterOperator notes)\n",
    "dagrun_data_dir = Path(\"../tests/\")\n",
    "static_dir = Path(\n",
    "    \"../tests\"\n",
    ")  # The path to the static_dir for this notebook (see FlowpyterOperator notes)\n",
    "JSON_DATA_SUBDIR = Path(\"jsons\")\n",
    "INDICATORS_TO_UPLOAD = [\n",
    "    \"residents.residents\",\n",
    "    \"residents.residents_perKm2\",\n",
    "    \"residents.arrived\",\n",
    "    \"residents.departed\",\n",
    "    \"residents.delta_arrived\",\n",
    "    \"residents.residents_diffwithref\",\n",
    "    \"residents.abnormality\",\n",
    "    \"residents.residents_pctchangewithref\",\n",
    "    \"relocations.relocations\",\n",
    "    \"relocations.relocations_diffwithref\",\n",
    "    \"relocations.abnormality\",\n",
    "    \"relocations.relocations_pctchangewithref\",\n",
    "    \"presence.presence\",\n",
    "    \"presence.presence_perKm2\",\n",
    "    \"presence.trips_in\",\n",
    "    \"presence.trips_out\",\n",
    "    \"presence.abnormality\",\n",
    "    \"presence.presence_diffwithref\",\n",
    "    \"presence.presence_pctchangewithref\",\n",
    "    \"movements.travellers\",\n",
    "    \"movements.abnormality\",\n",
    "    \"movements.travellers_diffwithref\",\n",
    "    \"movements.travellers_pctchangewithref\",\n",
    "]  # indicators to upload in the form 'category.indicator'\n",
    "CONFIG_STATIC_PATH = \"config.json\"  # The path to config.json within static_dir\n",
    "CHUNK_SIZE = 20  # Number of parallel uploads to attempt at once\n",
    "RETRY_COUNT = 3  # Number of times a chunk of parallel uploads will retry before failing\n",
    "BASE_URL = \"https://api.dev.haiti.mobility-dashboard.org/v1\"  # The base URL for the backend api\n",
    "MDIDS_DATA_PATH = Path(\"mdids\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d0d59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporary hack required until we update flowpyter-task to allow list params\n",
    "if isinstance(INDICATORS_TO_UPLOAD, str):\n",
    "    INDICATORS_TO_UPLOAD = json.loads(INDICATORS_TO_UPLOAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d8936c-c34a-44aa-a678-d9fc8b9cf31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Airflow variables injected via env vars\n",
    "AUTH0_CLIENT_ID_ADMIN = os.getenv(\"ADMIN_CLIENT\")  # Admin client id from Auth0\n",
    "AUTH0_CLIENT_ID_UPDATER = os.getenv(\"UPDATER_CLIENT\")  # Updator client id from Auth0\n",
    "AUTH0_CLIENT_SECRET_ADMIN = os.getenv(\"ADMIN_SECRET\")  # Admin secret from Auth0\n",
    "AUTH0_CLIENT_SECRET_UPDATER = os.getenv(\"UPDATER_SECRET\")  # Updator secret from Auth0\n",
    "AUTH0_DOMAIN = os.getenv(\n",
    "    \"AUTH0_DOMAIN\", \"flowminder-dev.eu.auth0.com\"\n",
    ")  # Auth0 domain to request tokens from\n",
    "AUDIENCE = os.getenv(\n",
    "    \"AUDIENCE\", \"https://flowkit-ui-backend.flowminder.org\"\n",
    ")  # Domain to request tokens for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3430de-f075-4eac-bb5d-ea65649960e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Postprocessing of parameters + af vars\n",
    "shared_data_dir = Path(shared_data_dir)\n",
    "dagrun_data_dir = Path(dagrun_data_dir)\n",
    "static_dir = Path(static_dir)\n",
    "JSON_FOLDER = dagrun_data_dir / JSON_DATA_SUBDIR\n",
    "CONFIG_PATH = static_dir / CONFIG_STATIC_PATH\n",
    "MDIDS_PATH = dagrun_data_dir / MDIDS_DATA_PATH\n",
    "CACHE_FOLDER = dagrun_data_dir / \"token_cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e26ab7-0100-478d-9602-727b66dbef5e",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_all_datasets_generator(\n",
    "    json_folder: Path, filename_pattern: str = \"*.json\"\n",
    ") -> Iterator[JsonDataset]:\n",
    "    for fp in json_folder.glob(filename_pattern):\n",
    "        data = json.loads(fp.read_text())\n",
    "        try:\n",
    "            yield JsonDataset(**data)\n",
    "        except TypeError as err:\n",
    "            if all(id not in data.keys() for id in (\"category_id\", \"indicator_id\")):\n",
    "                # Folder may contain json files that are not datasets - ignore them\n",
    "                log.debug(f\"{fp} is not a dataset, skipping\")\n",
    "            else:\n",
    "                # If json object has \"category_id\" and \"indicator_id\" keys then it _is_ a dataset,\n",
    "                # so if it's not a valid JsonDataset then it must be malformed\n",
    "                log.error(f\"{fp} contains a malformed dataset\")\n",
    "                raise (err)\n",
    "\n",
    "\n",
    "def filter_datasets_generator(\n",
    "    datasets: Iterator[JsonDataset], indicators_to_upload: list[str]\n",
    ") -> Iterator[JsonDataset]:\n",
    "    indicators_to_upload = set(indicators_to_upload)\n",
    "    present_indicators = set()\n",
    "    for ds in datasets:\n",
    "        # Yield only the indicators that are specified in indicators_to_upload\n",
    "        if ds.indicator_id in indicators_to_upload:\n",
    "            present_indicators.add(ds.indicator_id)\n",
    "            yield ds\n",
    "    # Warn if any of the specified indicators were not found\n",
    "    missing_indicators = indicators_to_upload - present_indicators\n",
    "    if missing_indicators:\n",
    "        log.warning(f\"No data found for the following indicators: {missing_indicators}\")\n",
    "    # Raise error if we did not yield any datasets at all\n",
    "    if not present_indicators:\n",
    "        raise UploadError(\"No indicators present\")\n",
    "\n",
    "\n",
    "async def main():\n",
    "    try:\n",
    "        response = httpx.get(f\"{BASE_URL}/heartbeat\", follow_redirects=True)\n",
    "    except TimeoutException:\n",
    "        # If we get a ReadTimeout, the last request caused the server to spin up. Try again now it's awake\n",
    "        await asyncio.sleep(\n",
    "            1\n",
    "        )  # Hey, if we're already async might as well be preemptible.\n",
    "        response = httpx.get(f\"{BASE_URL}/heartbeat\", follow_redirects=True)\n",
    "    if response.status_code >= 300:\n",
    "        raise HeartbeatError(\"Heartbeat not found\")\n",
    "\n",
    "    CACHE_FOLDER.mkdir(exist_ok=True)\n",
    "    admin_token = get_token(\n",
    "        AUTH0_DOMAIN,\n",
    "        AUTH0_CLIENT_ID_ADMIN,\n",
    "        AUTH0_CLIENT_SECRET_ADMIN,\n",
    "        AUDIENCE,\n",
    "        CACHE_FOLDER,\n",
    "    )\n",
    "    updater_token = get_token(\n",
    "        AUTH0_DOMAIN,\n",
    "        AUTH0_CLIENT_ID_UPDATER,\n",
    "        AUTH0_CLIENT_SECRET_UPDATER,\n",
    "        AUDIENCE,\n",
    "        CACHE_FOLDER,\n",
    "    )\n",
    "\n",
    "    responses = list(\n",
    "        get_remote_parameter(p.endpoint, admin_token, BASE_URL)\n",
    "        for p in [SridParams, TridParams, CategoryParams]\n",
    "    )\n",
    "    print([r for r in responses])\n",
    "    if any(r == [] for r in responses):\n",
    "        log.warning(\n",
    "            f\"Config not found: loading modifiers from {CONFIG_PATH} and uploading config\"\n",
    "        )\n",
    "        upload_config(CONFIG_PATH, admin_token, BASE_URL)\n",
    "\n",
    "    payloads = filter_datasets_generator(\n",
    "        load_all_datasets_generator(JSON_FOLDER), INDICATORS_TO_UPLOAD\n",
    "    )\n",
    "\n",
    "    # TODO maybe: Confirm or refetch srids/trids from server?\n",
    "    attempts = await do_upload(\n",
    "        payloads,\n",
    "        base_url=BASE_URL,\n",
    "        admin_token=updater_token,\n",
    "        chunk_size=CHUNK_SIZE,\n",
    "        retry_count=RETRY_COUNT,\n",
    "    )\n",
    "\n",
    "    mdids = (int(a.response.text) for a in attempts)\n",
    "    log.info(\"Setting 'read:preview_data' for uploads\")\n",
    "    responses = await set_scopes(mdids, \"read:preview_data\", BASE_URL, admin_token)\n",
    "    if any(r.status_code >= 300 for r in responses):\n",
    "        print([r.json() for r in responses])\n",
    "        raise UploadError(\"Scope setting failed\")\n",
    "\n",
    "    MDIDS_PATH.mkdir(exist_ok=True)\n",
    "    store_mdids(attempts, MDIDS_PATH)\n",
    "\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0efbef1-4eff-4ff1-b8b6-2ab99d617bdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
