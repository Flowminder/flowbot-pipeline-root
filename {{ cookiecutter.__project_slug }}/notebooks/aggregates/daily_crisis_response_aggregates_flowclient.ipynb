{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de8b66f2-bd57-4029-bb52-149e547cc3af",
   "metadata": {},
   "source": [
    "James Harrison, 2023-05-04\n",
    "\n",
    "This notebook is used to produce the following aggregates:\n",
    "- All-pairs trips OD matrix\n",
    "- Subscriber counts\n",
    "- Event counts\n",
    "- Active cell counts\n",
    "- Total active subscribers (admin0)\n",
    "\n",
    "for each day in the specified date range (by default, the most recently-ended full calendar month before today).\n",
    "\n",
    "No subscriber subsetting is used.\n",
    "\n",
    "These aggregates are intended to be produced on an ongoing basis in preparation for crisis response work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c1b1fc-c512-4cda-8fe6-8122b289aaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import flowclient as fc\n",
    "import flowmachine as fm\n",
    "import pandas as pd\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from flowclient import (\n",
    "    consecutive_trips_od_matrix,\n",
    "    location_event_counts,\n",
    "    total_network_objects,\n",
    "    trips_od_matrix,\n",
    "    unique_subscriber_counts,\n",
    ")\n",
    "from get_secret_or_env_var import environ\n",
    "from utils import (\n",
    "    find_dates_to_exclude,\n",
    "    get_date_in_month,\n",
    "    run_query_and_write_result,\n",
    "    run_query_and_write_result_async,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96e1e48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14443a00-5399-495a-94e5-c2942b10c93c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c5db11-1689-4cb2-b0ff-21dd8337238e",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cc4a7b-58aa-4aeb-9a65-9e86278e5017",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_now = datetime.datetime.now()\n",
    "datetime_now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf853b8-e10e-4c43-9a67-9fcd6ce6cce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All aggregates available to run using this notebook\n",
    "all_aggregates = (\n",
    "    \"subscriber-counts\",\n",
    "    \"all-trips\",\n",
    "    \"consecutive-trips\",\n",
    "    \"event-counts\",\n",
    "    \"active-cell-counts\",\n",
    "    \"total-active-subscribers\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d95bb8-dea3-44ba-b5b6-cb42eb18ea98",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "author = \"James Harrison <james.harrison@flowminder.org>\"\n",
    "\n",
    "start_date = get_date_in_month(\n",
    "    datetime_now, day_of_month=1, month_offset=-1\n",
    ")  # Start date of the data interval (inclusive)\n",
    "end_date = None  # End date of the data interval (exclusive) (defaults to one calendar month after start date)\n",
    "\n",
    "aggregation_unit = \"lon-lat\"  # Spatial aggregation unit\n",
    "mapping_table = \"geography.cell_to_admin_via_clusters_1km_20221025\"\n",
    "geom_table = \"geography.clusters_1km_20221025\"\n",
    "geom_table_join_column = \"cluster_id\"\n",
    "event_types = [\"calls\"]  # Event types to use\n",
    "\n",
    "shared_data_dir = \"./\"  # Writable output directory\n",
    "outputs_subdir = \"aggregates/crisis_response\"  # Subdirectory of shared data dir to which results of aggregate queries will be written\n",
    "output_format = \"csv\"  # 'csv' or 'netcdf'\n",
    "overwrite = False  # Set True to overwrite previously-saved aggregates for this month (with overwrite=False, conflicting aggregate files will be renamed)\n",
    "aggregates_to_calculate = all_aggregates\n",
    "require_latest_data = True  # If True, computation will not proceed if the last required day of data is later than the most recent available date\n",
    "use_async_client = False  # Set True to use the asynchronous flowclient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161bbd27-f2cc-494a-b2be-e18f1200d27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start/end date parameters may be strings, so convert to datetime.date\n",
    "start_date = pd.Timestamp(start_date).date()\n",
    "if end_date is None:\n",
    "    end_date = start_date + relativedelta(months=1)\n",
    "end_date = pd.Timestamp(end_date).date()\n",
    "\n",
    "(start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91aedd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct outputs path (we don't actually create the dir until we're ready to start writing outputs later)\n",
    "outputs_path = (\n",
    "    Path(shared_data_dir)\n",
    "    / outputs_subdir\n",
    "    / f\"daily_aggregates_{aggregation_unit}_{(end_date):%Y-%m-%d}\"\n",
    ")\n",
    "\n",
    "outputs_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5eddb00-2ee7-4600-9e00-49d2299e8678",
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_aggregates = set(aggregates_to_calculate).difference(all_aggregates)\n",
    "if unknown_aggregates:\n",
    "    raise ValueError(f\"Unknown aggregate types specified: {unknown_aggregates}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4018b3f3-4fa6-4b4f-8500-8810980a03bc",
   "metadata": {},
   "source": [
    "## Connect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a0ffbc-d504-4275-80a0-0cec45191c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_async_client:\n",
    "    fc_conn = await fc.connect_async(\n",
    "        url=environ[\"FLOWAPI_URL\"],\n",
    "        ssl_certificate=False,  # Workaround pending https://github.com/Flowminder/flowpyter-task/issues/35\n",
    "        token=environ[\"FLOWAPI_TOKEN\"],\n",
    "    )\n",
    "else:\n",
    "    fc_conn = fc.connect(\n",
    "        url=environ[\"FLOWAPI_URL\"],\n",
    "        ssl_certificate=False,  # Workaround pending https://github.com/Flowminder/flowpyter-task/issues/35\n",
    "        token=environ[\"FLOWAPI_TOKEN\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b6ccd2-40c9-432a-b795-70b6f3f48072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shouldn't need this, because we're not excluding dates based on temporal truncation,\n",
    "# but we still want to exclude based on missing dates, and we want to check the latest required date is available,\n",
    "# both of which are currently handled by `find_dates_to_exclude` which requires a db connection.\n",
    "fm.connect(\n",
    "    flowdb_connection_pool_overflow=20,\n",
    "    flowdb_connection_pool_size=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f18315e-f6e2-4a46-b161-cc69f83d6bce",
   "metadata": {},
   "source": [
    "## Check dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4bc884-bb8c-467f-b399-e80498eb4759",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_to_skip = find_dates_to_exclude(\n",
    "    flowdb_connection=fm.core.context.get_db(),\n",
    "    start_date=start_date,\n",
    "    end_date=end_date,\n",
    "    event_types=event_types,\n",
    "    latest_truncation_threshold=\"00:00:00\",  # Not excluding temporally-truncated data here\n",
    "    fail_on_missing_latest=require_latest_data,\n",
    ")\n",
    "dates_to_skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be37570-6ae8-4fbe-bd26-f3268096abad",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_to_run = sorted(\n",
    "    set(\n",
    "        str(d.date()) for d in pd.date_range(start_date, end_date, inclusive=\"left\")\n",
    "    ).difference(dates_to_skip)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bafa820-0462-4e0c-ab76-a7807826d0ef",
   "metadata": {
    "tags": []
   },
   "source": [
    "# FlowKit queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5a0317-ca5c-4c77-98c3-33342db440b5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b5d900-4f6b-444a-8c71-671dba16ce08",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_queries = {}\n",
    "for d in dates_to_run:\n",
    "    d_next = fm.utils.time_period_add(d, 1, \"days\")\n",
    "    common_args = dict(\n",
    "        connection=fc_conn,\n",
    "        start_date=d,\n",
    "        end_date=d_next,\n",
    "        aggregation_unit=aggregation_unit,\n",
    "        mapping_table=mapping_table,\n",
    "        geom_table=geom_table,\n",
    "        geom_table_join_column=geom_table_join_column,\n",
    "        event_types=event_types,\n",
    "    )\n",
    "    # Unique subscriber counts\n",
    "    if \"subscriber-counts\" in aggregates_to_calculate:\n",
    "        api_queries[f\"subscriber-counts_{d}\"] = unique_subscriber_counts(**common_args)\n",
    "    # Trips OD matrix (directed, all-pairs)\n",
    "    if \"all-trips\" in aggregates_to_calculate:\n",
    "        api_queries[f\"all-trips_{d}\"] = trips_od_matrix(**common_args)\n",
    "    # Consecutive trips OD matrix\n",
    "    if \"consecutive-trips\" in aggregates_to_calculate:\n",
    "        api_queries[f\"consecutive-trips_{d}\"] = consecutive_trips_od_matrix(\n",
    "            **common_args\n",
    "        )\n",
    "    # Event counts\n",
    "    if \"event-counts\" in aggregates_to_calculate:\n",
    "        api_queries[f\"event-counts_{d}\"] = location_event_counts(\n",
    "            **common_args,\n",
    "            count_interval=\"day\",\n",
    "        )\n",
    "    # Active cell counts\n",
    "    if \"active-cell-counts\" in aggregates_to_calculate:\n",
    "        api_queries[f\"active-cell-counts_{d}\"] = total_network_objects(\n",
    "            **common_args,\n",
    "            total_by=\"day\",\n",
    "        )\n",
    "    # Total active subscribers\n",
    "    if \"total-active-subscribers\" in aggregates_to_calculate:\n",
    "        api_queries[f\"total-active-subscribers_admin0_{d}\"] = unique_subscriber_counts(\n",
    "            connection=fc_conn,\n",
    "            start_date=d,\n",
    "            end_date=d_next,\n",
    "            aggregation_unit=\"admin0\",\n",
    "            mapping_table=mapping_table,\n",
    "            event_types=event_types,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88148602-ac8b-455e-a2f5-98fb2b3d5aaa",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Run queries and write results to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3475217e",
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_attrs = {\n",
    "    \"author\": author,\n",
    "    \"redacted\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98ead5f-5e18-4ad1-a3c6-029cda9eaa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_path.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7febb30e-0599-42f0-bdbf-7f235ee89354",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from types import coroutine\n",
    "from typing import Iterator\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class AggSpec:\n",
    "    filepath: Path\n",
    "    query: dict\n",
    "\n",
    "\n",
    "def agg_run_task(agg_spec: AggSpec) -> coroutine:\n",
    "    return run_query_and_write_result_async(\n",
    "        agg_spec.query,\n",
    "        filepath=agg_spec.filepath,\n",
    "        overwrite=overwrite,\n",
    "        file_format=output_format,\n",
    "        additional_attrs=additional_attrs,\n",
    "    )\n",
    "\n",
    "\n",
    "def agg_task_generator(agg_specs: list) -> Iterator[coroutine]:\n",
    "    for agg_spec in agg_specs:\n",
    "        yield agg_run_task(agg_spec)\n",
    "\n",
    "\n",
    "if use_async_client:\n",
    "    aggs_to_run = [\n",
    "        AggSpec(filepath=outputs_path / label, query=query)\n",
    "        for label, query in api_queries.items()\n",
    "    ]\n",
    "else:\n",
    "    # If using the sync client, we want to set all queries running before waiting on any results\n",
    "    for label, query in api_queries.items():\n",
    "        print(f\"Setting '{label}' query running...\")\n",
    "        query.run()\n",
    "    print(\"All queries are running\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc626719-7b6b-428b-b752-514cd20e9236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shamelessly nicked from the python3.12 docs\n",
    "from itertools import islice\n",
    "\n",
    "\n",
    "def batched(iterable, n):\n",
    "    # batched('ABCDEFG', 3) → ABC DEF G\n",
    "    if n < 1:\n",
    "        raise ValueError(\"n must be at least one\")\n",
    "    iterator = iter(iterable)\n",
    "    while batch := tuple(islice(iterator, n)):\n",
    "        yield batch\n",
    "\n",
    "\n",
    "if use_async_client:\n",
    "    import asyncio\n",
    "\n",
    "    retry_count = 3\n",
    "    batch_size = 10\n",
    "    exceptions = []\n",
    "    for attempt_no in range(0, retry_count):\n",
    "        print(\n",
    "            f\"Running queries, attempt {attempt_no}. {len(aggs_to_run)} aggregates to fetch.\"\n",
    "        )\n",
    "        for batch in batched(agg_task_generator(aggs_to_run), batch_size):\n",
    "            exceptions.append(await asyncio.gather(*batch, return_exceptions=True))\n",
    "        exceptions = [e for e in exceptions if e is not None]\n",
    "        if not exceptions:\n",
    "            print(\"All aggregates successfully fetched\")\n",
    "            break\n",
    "        aggs_to_run = [agg for agg in aggs_to_run if not agg.filepath.exists()]\n",
    "        print(f\"{len(aggs_to_run)} uploads failed\")\n",
    "        print([agg.filepath for agg in aggs_to_run])\n",
    "    if exceptions:\n",
    "        print(exceptions)\n",
    "        raise Exception(\n",
    "            f\"The following uploads failed: {[ agg.filepath for agg in aggs_to_run ]}\"\n",
    "        )\n",
    "\n",
    "else:\n",
    "    for label, query in api_queries.items():\n",
    "        print(f\"Getting result of '{label}' query...\")\n",
    "        run_query_and_write_result(\n",
    "            query,\n",
    "            filepath=outputs_path / label,\n",
    "            overwrite=overwrite,\n",
    "            file_format=output_format,\n",
    "            additional_attrs=additional_attrs,\n",
    "        )\n",
    "\n",
    "print(\"All queries completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc89c6a-8250-45da-aff5-644294a31d19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
