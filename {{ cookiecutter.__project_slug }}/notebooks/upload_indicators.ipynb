{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f158709",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fb107d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e268d81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e58c39-0b76-4e3c-82e0-7420d339c4c3",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "from pathlib import Path\n",
    "from httpx import ReadTimeout, TimeoutException\n",
    "import os\n",
    "import httpx\n",
    "import logging\n",
    "import csv\n",
    "from functools import partial\n",
    "\n",
    "from data_cleaning import (\n",
    "    csv_to_dataset,\n",
    "    RelocationsDataset,\n",
    "    ResidentsDataset,\n",
    "    MovementsDataset,\n",
    "    PresenceDataset,\n",
    ")\n",
    "from auth import get_token\n",
    "from params import (\n",
    "    SridParams,\n",
    "    TridParams,\n",
    "    CategoryParams,\n",
    "    lookup_factory,\n",
    "    upload_config,\n",
    "    get_remote_parameter,\n",
    "    get_local_parameter,\n",
    ")\n",
    "from upload import do_upload, AttemptState, set_scopes, store_mdids\n",
    "from pandas import DataFrame, read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c921b66-4b12-4034-8a22-c7719475228f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UploadError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "class InvalidIndicatorError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "class HeartbeatError(Exception):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c41419-df25-46dd-81dd-077bd40a7772",
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in Path(\"../.env\").read_text().splitlines():\n",
    "    foo, _, bar = line.partition(\"=\")\n",
    "    os.environ[foo] = bar\n",
    "\n",
    "log = logging.getLogger(\"upload_notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13398e18-5fd1-4c89-9b91-7a289dc6ed82",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "shared_data_dir = Path(\n",
    "    \"../tests/\"\n",
    ")  # The path to the shared_data_dir for this notebook (see FlowpyterOperator notes)\n",
    "dagrun_data_dir = Path(\"../tests/\")\n",
    "static_dir = Path(\n",
    "    \"../tests\"\n",
    ")  # The path to the static_dir for this notebook (see FlowpyterOperator notes)\n",
    "INDICATORS_LIST = list(\n",
    "    (static_dir / \"sample_indicators\").glob(\"*.csv\")\n",
    ")  # A list of path-formatted strings relative to shared_data_dir of indicator csvs to upload\n",
    "CONFIG_STATIC_PATH = \"config.json\"  # The path to config.json within static_dir\n",
    "CHUNK_SIZE = 20  # Number of parallel uploads to attempt at once\n",
    "RETRY_COUNT = 3  # Number of times a chunk of parallel uploads will retry before failing\n",
    "DATA_VERSION = os.getenv(\"DATA_VERSION\")  # The data version of this upload\n",
    "JSON_DATA_SUBDIR = \"outputs\"  # If set, the path within dagrun_data_dir to cache json upload artefacts. Can be None.\n",
    "REDACTED_ADMIN_3_STATIC_PATH = \"redacted_sections.csv\"  # The path to the .csv containing a column 'pcod' of admin3 regions to redact\n",
    "BASE_URL = \"https://api.dev.haiti.mobility-dashboard.org/v1\"  # The base URL for the backend api\n",
    "MDIDS_DATA_PATH = \"mdids\"\n",
    "CDR_POPULATION_FILE = \"cdr_pop.csv\"  # Name of file containing CDR-derived subscriber population counts (within dagrun dir). Should have columns 'pcod' (admin3) and 'value' (subscriber population count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d8936c-c34a-44aa-a678-d9fc8b9cf31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Airflow variables injected via env vars\n",
    "AUTH0_CLIENT_ID_ADMIN = os.getenv(\"ADMIN_CLIENT\")  # Admin client id from Auth0\n",
    "AUTH0_CLIENT_ID_UPDATER = os.getenv(\"UPDATER_CLIENT\")  # Updator client id from Auth0\n",
    "AUTH0_CLIENT_SECRET_ADMIN = os.getenv(\"ADMIN_SECRET\")  # Admin secret from Auth0\n",
    "AUTH0_CLIENT_SECRET_UPDATER = os.getenv(\"UPDATER_SECRET\")  # Updator secret from Auth0\n",
    "AUTH0_DOMAIN = os.getenv(\n",
    "    \"AUTH0_DOMAIN\", \"flowminder-dev.eu.auth0.com\"\n",
    ")  # Auth0 domain to request tokens from\n",
    "AUDIENCE = os.getenv(\n",
    "    \"AUDIENCE\", \"https://flowkit-ui-backend.flowminder.org\"\n",
    ")  # Domain to request tokens for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3430de-f075-4eac-bb5d-ea65649960e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Postprocessing of parameters + af vars\n",
    "shared_data_dir = Path(shared_data_dir)\n",
    "dagrun_data_dir = Path(dagrun_data_dir)\n",
    "static_dir = Path(static_dir)\n",
    "JSON_FOLDER = dagrun_data_dir / JSON_DATA_SUBDIR\n",
    "REDACTED_ADMIN_3_LIST = static_dir / REDACTED_ADMIN_3_STATIC_PATH\n",
    "CONFIG_PATH = static_dir / CONFIG_STATIC_PATH\n",
    "INDICATORS_PATHS = [shared_data_dir / ind for ind in INDICATORS_LIST]\n",
    "MDIDS_PATH = static_dir / MDIDS_DATA_PATH\n",
    "CACHE_FOLDER = dagrun_data_dir / \"token_cache\"\n",
    "CDR_POPULATION_PATH = dagrun_data_dir / CDR_POPULATION_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ebaefe-007e-4044-80c6-13f54d6da11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in INDICATORS_PATHS:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aad1a26-9065-49dc-a36e-2b6f4ee56c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redaction rules\n",
    "def get_redaction_list(redaction_list_path):\n",
    "    with open(redaction_list_path, \"r\") as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        return [row[\"pcod\"] for row in reader]\n",
    "\n",
    "\n",
    "def cdr_pop_redaction_residents(df: DataFrame) -> DataFrame:\n",
    "    cdr_pop = read_csv(CDR_POPULATION_PATH)\n",
    "    admin3s_above_threshold = cdr_pop.loc[cdr_pop[\"value\"] >= 200, \"pcod\"]\n",
    "    return df.loc[\n",
    "        df.index.get_level_values(\"spatial_unit\").isin(admin3s_above_threshold)\n",
    "    ]\n",
    "\n",
    "\n",
    "def cdr_pop_redaction_relocations(df: DataFrame) -> DataFrame:\n",
    "    cdr_pop = read_csv(CDR_POPULATION_PATH)\n",
    "    admin3s_above_threshold = cdr_pop.loc[cdr_pop[\"value\"] >= 200, \"pcod\"]\n",
    "    return df.loc[\n",
    "        df.index.get_level_values(\"origin\").isin(admin3s_above_threshold)\n",
    "        & df.index.get_level_values(\"destination\").isin(admin3s_above_threshold)\n",
    "    ]\n",
    "\n",
    "\n",
    "def admin_3_drop_residents(df: DataFrame) -> DataFrame:\n",
    "    return df.drop(get_redaction_list(REDACTED_ADMIN_3_LIST), level=1, errors=\"ignore\")\n",
    "\n",
    "\n",
    "def admin_3_drop_relocations(df: DataFrame) -> DataFrame:\n",
    "    df = df.drop(get_redaction_list(REDACTED_ADMIN_3_LIST), level=1, errors=\"ignore\")\n",
    "    return df.drop(get_redaction_list(REDACTED_ADMIN_3_LIST), level=2, errors=\"ignore\")\n",
    "\n",
    "\n",
    "def relocations_redaction(df: DataFrame) -> DataFrame:\n",
    "    indicies = df.loc[df[\"relocations\"] <= 15].index.values\n",
    "    return df.drop(indicies)\n",
    "\n",
    "\n",
    "def residents_redaction(df: DataFrame) -> DataFrame:\n",
    "    indicies = df.loc[df[\"residents\"] <= 15].index.values\n",
    "    return df.drop(indicies)\n",
    "\n",
    "\n",
    "def presence_redaction(df: DataFrame) -> DataFrame:\n",
    "    indicies = df.loc[df[\"presence\"] <= 15].index.values\n",
    "    return df.drop(indicies)\n",
    "\n",
    "\n",
    "def trips_redaction(df: DataFrame) -> DataFrame:\n",
    "    indicies = df.loc[df[\"trips\"] <= 15].index.values\n",
    "    return df.drop(indicies)\n",
    "\n",
    "\n",
    "def arrivals_departed_nan(df: DataFrame) -> DataFrame:\n",
    "    return df.dropna(subset=[\"arrived\", \"departed\"])\n",
    "\n",
    "\n",
    "def not_implemented_redaction(df):\n",
    "    raise NotImplementedError(\"Redactions not implemented for this dataset\")\n",
    "\n",
    "\n",
    "def round_residents(df) -> DataFrame:\n",
    "    col_dps = {\n",
    "        \"residents\": -2,\n",
    "        \"residents_perKm2\": -1,\n",
    "        \"arrived\": -1,\n",
    "        \"departed\": -1,\n",
    "        \"delta_arrived\": -1,\n",
    "        \"residents_diffwithref\": -1,\n",
    "        \"abnormality\": 3,\n",
    "        \"residents_pctchangewithref\": 2,\n",
    "    }\n",
    "    return df.round(col_dps)\n",
    "\n",
    "\n",
    "def round_relocations(df) -> DataFrame:\n",
    "    col_dps = {\n",
    "        \"relocations\": -1,\n",
    "        \"relocations_diffwithref\": -1,\n",
    "        \"abnormality\": 3,\n",
    "        \"relocations_pctchangewithref\": 2,\n",
    "    }\n",
    "    return df.round(col_dps)\n",
    "\n",
    "\n",
    "def round_movements(df) -> DataFrame:\n",
    "    col_dps = {\n",
    "        \"trips\": -2,\n",
    "        \"abnormality\": 3,\n",
    "        \"trips_diffwithref\": -1,\n",
    "        \"trips_pctchangewithref\": 2,\n",
    "    }\n",
    "    return df.round(col_dps)\n",
    "\n",
    "\n",
    "def round_presence(df) -> DataFrame:\n",
    "    col_dps = {\n",
    "        \"presence\": -2,\n",
    "        \"presence_perKm2\": -1,\n",
    "        \"trips_in\": -1,\n",
    "        \"trips_out\": -1,\n",
    "        \"abnormality\": 3,\n",
    "        \"presence_diffwithref\": -1,\n",
    "        \"presence_pxtchangewithref\": 2,\n",
    "    }\n",
    "    return df.round(col_dps)\n",
    "\n",
    "\n",
    "RESIDENTS_REDACTIONS = (\n",
    "    admin_3_drop_residents,\n",
    "    cdr_pop_redaction_residents,\n",
    "    residents_redaction,\n",
    "    arrivals_departed_nan,\n",
    "    round_residents,\n",
    ")\n",
    "\n",
    "RELOCATIONS_REDACTIONS = (\n",
    "    admin_3_drop_relocations,\n",
    "    cdr_pop_redaction_relocations,\n",
    "    relocations_redaction,\n",
    "    round_relocations,\n",
    ")\n",
    "\n",
    "PRESENCE_REDACTIONS = (\n",
    "    admin_3_drop_residents,\n",
    "    cdr_pop_redaction_residents,\n",
    "    presence_redaction,\n",
    "    round_presence,\n",
    ")\n",
    "\n",
    "MOVEMENTS_REDACTIONS = (\n",
    "    admin_3_drop_relocations,\n",
    "    cdr_pop_redaction_relocations,\n",
    "    trips_redaction,\n",
    "    round_movements,\n",
    ")\n",
    "\n",
    "\n",
    "def redactor_factory(redaction_rules) -> callable:\n",
    "    def inner(df: DataFrame):\n",
    "        for rule in redaction_rules:\n",
    "            df = rule(df)\n",
    "        return df\n",
    "\n",
    "    return inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e26ab7-0100-478d-9602-727b66dbef5e",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def trid_lookup(trid_label):\n",
    "    trid_dict = {\"years\": 17, \"months\": 18, \"weeks\": 19, \"days\": 20}\n",
    "    return trid_dict[trid_label]\n",
    "\n",
    "\n",
    "def srid_lookup(srid_label):\n",
    "    srid_dict = {\"Commune\": 14, \"Communal section\": 15, \"Department\": 13}\n",
    "    return srid_dict[srid_label]\n",
    "\n",
    "\n",
    "def category_lookup(cat_label):\n",
    "    cat_dict = {\n",
    "        \"relocations\": \"flow\",\n",
    "        \"residents\": \"single_location\",\n",
    "        \"presence\": \"single_location\",\n",
    "        \"movements\": \"flow\",\n",
    "    }\n",
    "    return cat_dict[cat_label]\n",
    "\n",
    "\n",
    "async def main():\n",
    "    try:\n",
    "        response = httpx.get(f\"{BASE_URL}/heartbeat\", follow_redirects=True)\n",
    "    except TimeoutException:\n",
    "        # If we get a ReadTimeout, the last request caused the server to spin up. Try again now it's awake\n",
    "        await asyncio.sleep(\n",
    "            1\n",
    "        )  # Hey, if we're already async might as well be preemptible.\n",
    "        response = httpx.get(f\"{BASE_URL}/heartbeat\", follow_redirects=True)\n",
    "    if response.status_code >= 300:\n",
    "        raise HeartbeatError(\"Heartbeat not found\")\n",
    "\n",
    "    CACHE_FOLDER.mkdir(exist_ok=True)\n",
    "    admin_token = get_token(\n",
    "        AUTH0_DOMAIN,\n",
    "        AUTH0_CLIENT_ID_ADMIN,\n",
    "        AUTH0_CLIENT_SECRET_ADMIN,\n",
    "        AUDIENCE,\n",
    "        CACHE_FOLDER,\n",
    "    )\n",
    "    updater_token = get_token(\n",
    "        AUTH0_DOMAIN,\n",
    "        AUTH0_CLIENT_ID_UPDATER,\n",
    "        AUTH0_CLIENT_SECRET_UPDATER,\n",
    "        AUDIENCE,\n",
    "        CACHE_FOLDER,\n",
    "    )\n",
    "\n",
    "    responses = list(\n",
    "        get_remote_parameter(p.endpoint, admin_token, BASE_URL)\n",
    "        for p in [SridParams, TridParams, CategoryParams]\n",
    "    )\n",
    "    print([r for r in responses])\n",
    "    if any(r == [] for r in responses):\n",
    "        log.warning(\n",
    "            f\"Config not found: loading modifiers from {CONFIG_PATH} and uploading config\"\n",
    "        )\n",
    "        upload_config(CONFIG_PATH, admin_token, BASE_URL)\n",
    "        param_fetcher = partial(get_local_parameter, config_path=CONFIG_PATH)\n",
    "    else:\n",
    "        param_fetcher = partial(\n",
    "            get_remote_parameter, admin_token=admin_token, base_url=BASE_URL\n",
    "        )\n",
    "\n",
    "    for indicator in INDICATORS_PATHS:\n",
    "        log.info(f\"Uploading {indicator}\")\n",
    "        if \"residents\" in indicator.name:\n",
    "            csv_ds = ResidentsDataset(indicator)\n",
    "            local_redactor = redactor_factory(RESIDENTS_REDACTIONS)\n",
    "        elif \"relocations\" in indicator.name:\n",
    "            csv_ds = RelocationsDataset(indicator)\n",
    "            local_redactor = redactor_factory(RELOCATIONS_REDACTIONS)\n",
    "        elif \"movements\" in indicator.name:\n",
    "            csv_ds = MovementsDataset(indicator)\n",
    "            local_redactor = redactor_factory(MOVEMENTS_REDACTIONS)\n",
    "        elif \"presence\" in indicator.name:\n",
    "            csv_ds = PresenceDataset(indicator)\n",
    "            local_redactor = redactor_factory(PRESENCE_REDACTIONS)\n",
    "        else:\n",
    "            raise InvalidIndicatorError(\n",
    "                f\"Invalid indicator {indicator.name}; must include one of 'residents', 'relocations', 'movements' or 'presence'.\"\n",
    "            )\n",
    "        payloads = csv_to_dataset(\n",
    "            csv_ds,\n",
    "            srid_lookup=srid_lookup,\n",
    "            trid_lookup=trid_lookup,\n",
    "            category_type_lookup=category_lookup,\n",
    "            redactor=local_redactor,\n",
    "            revision=DATA_VERSION,\n",
    "        )\n",
    "\n",
    "        attempts = await do_upload(\n",
    "            payloads, BASE_URL, updater_token, JSON_FOLDER, CHUNK_SIZE, RETRY_COUNT\n",
    "        )\n",
    "\n",
    "        if any(a.state == AttemptState.FAILED for a in attempts):\n",
    "            raise UploadError(\"Some uploads failed, see logs\")\n",
    "\n",
    "        mdids = (int(a.response.text) for a in attempts)\n",
    "        log.info(\"Setting 'read:preview_data' for uploads\")\n",
    "        responses = await set_scopes(mdids, \"read:preview_data\", BASE_URL, admin_token)\n",
    "        if any(r.status_code >= 300 for r in responses):\n",
    "            print([r.json() for r in responses])\n",
    "            raise UploadError(\"Scope setting failed\")\n",
    "\n",
    "        store_mdids(attempts, MDIDS_PATH)\n",
    "\n",
    "\n",
    "await main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
